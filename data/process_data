#!/usr/bin/env python
#####################
## by Lucas Parzianello - University of Notre Dame
## PROCESS INPUT DATA
## Users, papers and their relations:
# Converts .gdf to .json
# Rank dataset
# Paired comparisons
# Clustering

from array import array
import json
import operator
import tokenize
import re
import sys
import os

###############
### PARAMETERS:

# general:
nParticipants = 145
nArticles = 183
gdf_name = "bipartite_nodes_CSCW2013_URL_time.gdf"  # input file name
json_name = "data.json"                             # JSON output file name
programDir = "clustering/program/"                   # directory containing ap exec

# file names
simUsers_name = "clustering/users/SimUsers.txt"         # Similarity output file name
simPapers_name = "clustering/papers/SimPapers.txt"
prefUsers_name = "clustering/users/PrefUsers.txt"       # Preferences output file name
prefPapers_name = "clustering/papers/PrefPapers.txt"
mapUsers_name = "clustering/users/MapUsers.txt"         # Map file relating seq ids to original ones
mapPapers_name = "clustering/papers/MapPapers.txt"
binUsers_name = "clustering/users/users.bin"			# Binary files to clustering
binPapers_name = "clustering/papers/papers.bin"

# delimiters
rDel = None# Paired comparisons
pDel = nParticipants
aDel = nParticipants + nArticles

# weighted link between a and b
class Link:
    a = ""
    b = ""
    weight = 0
    def __init__(self, a, b, weight):
        self.a = a
        self.b = b
        self.weight = weight

########################
### AUXILIARY FUNCTIONS:

# import data from .gdf
def importData (gdf_file, write) :

    content = gdf_file.readlines()
    json_data, elem_list, part, data, art, rel = ({} for i in range(6))
    part_list, art_list, rel_list, cluster_data = ([] for i in range(4))

    rDel = len(content)

    for i,c in enumerate(content):
        if (i <= pDel and i > 0):
            elem_list = c.rstrip("'\n").rstrip("'").split("','");

            if(len(elem_list) == 6):
                part = {}
                data = {}
                data['label'] = elem_list[1]
                data['labelShort'] = elem_list[2]
                data['url'] = elem_list[4]
                data['date'] = elem_list[5]
                part['name'] = "user."+elem_list[3] + "." + elem_list[0][1:]
                part['data'] = data
                part['imports'] = []
                part_list.append(part)

        elif (i > nParticipants and i <= aDel) :
            elem_list = c.rstrip("'\n").rstrip("'").split("','");

            if(len(elem_list) == 6):
                art = {}
                data = {}
                data['label'] = elem_list[1]
                data['labelShort'] = elem_list[2]
                data['url'] = elem_list[4]
                data['date'] = elem_list[5]
                art['name'] = "paper."+elem_list[3] + "." + elem_list[0][1:]
                art['data'] = data
                art['imports'] = []
                art_list.append(art)

        elif (i > nParticipants+nArticles+1 and i <= rDel) :
            elem_list = c.rstrip("'\n").rstrip("'").split("','");

            if(len(elem_list) == 3):
                rel = {}
                rel['relUser'] = elem_list[0][1:]
                rel['relPaper'] = elem_list[1]
                rel['relTime'] = elem_list[2]

                userType = "user."
                paperType = "talk."
                currentUser = "user." + userType + elem_list[0][1:]
                currentPaper = "paper." + paperType + elem_list[1]

                # create entries to 'imports' in users and papers
                for e in part_list:
                    if (e['name'] == currentUser):
                        e['imports'].append(currentPaper)
                for e in art_list:
                    if (e['name'] == currentPaper):
                        e['imports'].append(currentUser)

                rel_list.append(rel)

    if(write):
        json_file = open(json_name, "w")
        json_file.write( json.dumps(part_list+art_list, json_file, indent=4) )

    return (part_list, art_list, rel_list)

# maps any ID dictionary into a dictionary with sequential keys starting from 1
def mapping (id_dict):
    new_id_dict = {}
    mapping_string = ""
    for i,u in enumerate(id_dict):
        oldid = u['name'].split('.')[-1]
        new_id_dict[oldid] = i+1
        mapping_string += oldid + ' ' + str(new_id_dict[oldid]) + '\n'

    return (new_id_dict, mapping_string)

# given a dictionary and a value, retrieves first key (k)
def retrieveKey (dictio, value):
    value = int(value)
    for k, v in dictio.iteritems():
        if v == value:
            return k
    return None

# returns a list of [item, count] given a dictionary and a field
def counting (dictio, field, pTop=0):
    unsortedCount = {}
    for r in dictio:
        key = str(r[field])
        if(not (key in unsortedCount)):
            unsortedCount[key] = 1
        else:
            unsortedCount[key] += 1
    count = sorted(unsortedCount.items(), key=operator.itemgetter(1), reverse=True)
    for c in range(0, pTop):
        print('\tItem ' + count[c][0] + ' has ' + str(count[c][1]) + ' objects connected to ')
    print("")

    return count

# for every pair within lis elements, counts how many items in 'importfield' they share
def countPairs (lis, lookup, namefield='name', importfield='imports', pTop=0):
    sim = pref = ""
    size = len(lis)
    pairs = []
    aff = [[0 for x in range(size)] for x in range(size)]
    for ui1 in range(0, size):
        for ui2 in range(ui1+1, size):
            i1 = lis[ui1]
            i2 = lis[ui2]
            n1 = lookup[ i1[namefield].split('.')[-1] ]
            n2 = lookup[ i2[namefield].split('.')[-1] ]

            inter = set(i1[importfield]).intersection(i2[importfield])
            sizeInter = len(inter)
            if(size != 0) :
                pairs.append( Link(a=str(n1), b=str(n2), weight=sizeInter) )
                sim += str(n1) + ' ' + str(n2) + ' ' + str(sizeInter*-1) + '\n'
                aff[n1-1][n2-1] = -1*sizeInter
                aff[n2-1][n1-1] = -1*sizeInter
                aff[n1-1][n1-1] = -1
                aff[n2-1][n2-1] = -1
            pref += "-1\n"
    # sorts the pairs list
    pairs.sort(key=lambda x: x.weight, reverse=True)

    for c in range (0,pTop):
        if (pairs[c].weight):
            print('\t'+str(c+1)+': '+retrieveKey(lookup, pairs[c].a)+' and '+retrieveKey(lookup, pairs[c].b)+' have '+str(pairs[c].weight) + ' items in common')
    print("")

    return (pairs, sim, pref, aff)

# writes to file (oh!)
def writeToFile (output, file_name):
    with open(file_name, "w") as file:
	    file.write(output)

# form a file with clustered values and a dictionary, outputs the "translated" clusters using this dictionary. Note: e[:-1] gets rid of '\n' character at the end
def convertClusters (cluster_file, id_dict):
    ids = cluster_file.readlines()
    cluster_file.close()
    newCluster = {}
    for i,e in enumerate(ids):
        newid = retrieveKey(id_dict, i+1)
        newCluster.setdefault(retrieveKey(id_dict, e[:-1]), []).append(newid)
    return newCluster

# opens index files (used here to read clustering program output)
def readIdxFiles ():
    idx_file = None
    while (not idx_file):
        try:
            idxUsers_file = open(binUsers_name + "_out.txt","r")
            idxPapers_file = open(binPapers_name + "_out.txt","r")
        except:
            raw_input()
            continue
        return (idxUsers_file, idxPapers_file)

# outputs a binary file (length + length X length matrix using floats 32bits)
def writeToBinFile (values, file_name):
	size = len(values[0])
	values = sum(values, [])
	values.insert(0,size)
	def splitNumber (num):		# in case num is larger than 1 byte (0-255)
		num = map(int, str(num))
		lst = []
		while num > 0:
		    lst.append(num & 0xFF)
		    num >>= 8
		return lst[::-1]

	with open(file_name, 'wb') as f:
		values = array('f', values)
		values.tofile(f)

	sys.stdout.write("First 10#: ")
	for i in range(0,10):
		sys.stdout.write(str(values[i])+' ')
	print("")

# Runs kmeans clustering over data in file of 'binary_name'.
# Verbose output in 'binary_name.log'
def kmeansClustering (binary_name, number_of_clusters):
    executable = programDir + "kmeans_cluster"
    if(os.system("./"+executable+" "+str(number_of_clusters)+" "+binary_name+" > "+binary_name+".log")):
        print("Error executing clustering\nMore info in: "+binary_name+".log")
        return False
    return True

def updateJSON (allClusters):
    def getKey(value):
        for k, v in allClusters.iteritems():
            for n in v:
                if n == value:
                    return k
        return None

    json_file = open(json_name, "r+")
    json_obj = json.loads(json_file.read())
    for o in json_obj:
        imports = []
        n = o["name"].split('.')
        o["name"] = n[0]+"."+getKey(n[-1])+"."+n[1]+"."+n[2]
        for i in o['imports']:
            cluster = getKey(i.split('.')[-1])
            line = i.split('.')
            imports.append(line[0]+"."+cluster+"."+line[1]+"."+line[2])
        o['imports'] = imports
    json_file.seek(0)
    json_file.write( json.dumps(json_obj, json_file, indent=4) )
    json_file.truncate()

####################
### EXPLORE THE DATA
def similarity (gdf_file) :

    part_list, art_list, rel_list = importData (gdf_file, write=True)

    # Mapping
    newUid, mapUsers = mapping (part_list)
    newPid, mapPapers = mapping (art_list)

    # Count singles
    uSingles = counting (rel_list, 'relUser', pTop=10)
    pSingles = counting (rel_list, 'relPaper', pTop=10)

    # Count pairs
    uPairs,simUsers,prefUsers,affinityUsers = countPairs (lis=part_list,lookup=newUid, pTop=10)
    pPairs,simPapers,prefPapers,affinityPapers = countPairs (lis=art_list,lookup=newPid, pTop=10)

    # Write txt files
    writeToFile (mapUsers,   mapUsers_name)
    writeToFile (mapPapers,  mapPapers_name)
    writeToFile (prefUsers,  prefUsers_name)
    writeToFile (prefPapers, prefPapers_name)
    writeToFile (simUsers,   simUsers_name)
    writeToFile (simPapers,  simPapers_name)

	# Write binary files
    writeToBinFile (affinityUsers,  binUsers_name)
    writeToBinFile (affinityPapers, binPapers_name)

    # Run kmeans program with given parameters
    kmeansClustering(binPapers_name, number_of_clusters = 8)
    kmeansClustering(binUsers_name, number_of_clusters = 3)

    # Read clustering _out.txt files and get the clusters
    idxUsers_file, idxPapers_file = readIdxFiles()
    userClusters = convertClusters(cluster_file=idxUsers_file, id_dict=newUid)
    paperClusters = convertClusters(cluster_file=idxPapers_file, id_dict=newPid)

    print("\n# paper clusters: "+str(len(paperClusters))+'\n# user clusters: '+str(len(userClusters))+'\n')

    allClusters = {}
    allClusters.update(userClusters)
    allClusters.update(paperClusters)
    updateJSON(allClusters)

    return True

#########
### MAIN:
def main () :

    gdf_file = open(gdf_name, "r")

    if(similarity(gdf_file)) :
        print ("Processing successfully completed.")
    else:
        print (">>> Error processing data <<<")

main()
